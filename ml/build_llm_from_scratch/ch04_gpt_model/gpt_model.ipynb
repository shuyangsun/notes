{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2550f87b-516e-4daf-8a14-96bcada918f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import util\n",
    "\n",
    "from typing import Self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe47ed-b6b8-417c-a91a-d7b5e600daba",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94ac3e1-78c8-4dec-85b6-940b3d7bf68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        context_length: int,\n",
    "        emb_dim: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        drop_rate: float,\n",
    "        qkv_bias: bool\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_rate = drop_rate\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "    def as_dict(self) -> dict[str, int | float | bool]:\n",
    "        return self.__dict__\n",
    "\n",
    "    @classmethod\n",
    "    def gpt2_small(cls) -> Self:\n",
    "        return Config(\n",
    "            vocab_size = 50257,\n",
    "            context_length = 1024,\n",
    "            emb_dim = 768,\n",
    "            n_heads = 12,\n",
    "            n_layers = 12,\n",
    "            drop_rate = 0.1,\n",
    "            qkv_bias = False,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def gpt2_medium(cls) -> Self:\n",
    "        return Config(\n",
    "            vocab_size = 50257,\n",
    "            context_length = 1024,\n",
    "            emb_dim = 1024,\n",
    "            n_heads = 16,\n",
    "            n_layers = 24,\n",
    "            drop_rate = 0.1,\n",
    "            qkv_bias = False,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def gpt2_large(cls) -> Self:\n",
    "        return Config(\n",
    "            vocab_size = 50257,\n",
    "            context_length = 1024,\n",
    "            emb_dim = 1280,\n",
    "            n_heads = 20,\n",
    "            n_layers = 36,\n",
    "            drop_rate = 0.1,\n",
    "            qkv_bias = False,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def gpt2_xl(cls) -> Self:\n",
    "        return Config(\n",
    "            vocab_size = 50257,\n",
    "            context_length = 1024,\n",
    "            emb_dim = 1600,\n",
    "            n_heads = 25,\n",
    "            n_layers = 48,\n",
    "            drop_rate = 0.1,\n",
    "            qkv_bias = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7982d159-c440-452a-8712-b6e500cfacca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.gpt2_small().as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7d79e-44bf-445b-a3fa-a423bbd87080",
   "metadata": {},
   "source": [
    "## Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db866c0-ea25-4308-b829-38a38c68b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-7\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063fe1d0-6466-4fa8-b536-02c084a2def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim: int, expand_factor: int = 4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, expand_factor * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expand_factor * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2404a1b-ebcc-4e9c-8944-a877451eea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.mhatt = util.MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            dropout=cfg.drop_rate,\n",
    "            num_heads=cfg.n_heads,\n",
    "        )\n",
    "        self.ff = FeedForward(cfg.emb_dim)\n",
    "        self.norm1 = LayerNorm(cfg.emb_dim)\n",
    "        self.norm2 = LayerNorm(cfg.emb_dim)\n",
    "        self.drop = nn.Dropout(cfg.drop_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.drop(self.mhatt(self.norm1(x))) + x\n",
    "        return self.drop(self.ff(self.norm2(x))) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091ed315-fd67-4862-9316-8096b036605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = TransformerBlock(Config.gpt2_small())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333dfb63-861a-4dc3-bd33-3634bad72863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_tensor = torch.rand(1, 1024, 768)\n",
    "block(in_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80d45ae-3a36-47ef-91e3-e2393a2cada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.drop_rate)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, seq_len = x.shape\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        return self.out_head(self.final_norm(self.trf_blocks(self.drop_emb(tok_emb + pos_emb))))\n",
    "\n",
    "    def num_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def f32_param_size_gb(self) -> int:\n",
    "        return self.num_params() * 4 / 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6087fca8-173d-4319-a8dd-31e6c018ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small = GPTModel(Config.gpt2_small())\n",
    "gpt2_medium = GPTModel(Config.gpt2_medium())\n",
    "gpt2_large = GPTModel(Config.gpt2_large())\n",
    "gpt2_xl = GPTModel(Config.gpt2_xl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd309c5-472c-484a-9e92-b8ba1f1526d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Small: 163,009,536 params, 0.61 GB\n",
      "GPT-2 Medium: 406,212,608 params, 1.51 GB\n",
      "GPT-2 Large: 838,220,800 params, 3.12 GB\n",
      "GPT-2 XL: 1,637,792,000 params, 6.10 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'GPT-2 Small: {gpt2_small.num_params():,} params, {gpt2_small.f32_param_size_gb():,.2f} GB')\n",
    "print(f'GPT-2 Medium: {gpt2_medium.num_params():,} params, {gpt2_medium.f32_param_size_gb():,.2f} GB')\n",
    "print(f'GPT-2 Large: {gpt2_large.num_params():,} params, {gpt2_large.f32_param_size_gb():,.2f} GB')\n",
    "print(f'GPT-2 XL: {gpt2_xl.num_params():,} params, {gpt2_xl.f32_param_size_gb():,.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27ba7a5-6e48-4871-b973-b32680276b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 1024])\n",
      "Output shape: torch.Size([8, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokenizer: tiktoken.Encoding = tiktoken.get_encoding('gpt2')\n",
    "data_loader = util.create_dataloader_v1(\n",
    "    content=util.text_corpus(),\n",
    "    batch_size=8,\n",
    "    context_window=Config.gpt2_small().context_length,\n",
    "    stride=Config.gpt2_small().context_length // 2,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "x, y = next(data_iter)\n",
    "print(f'Input shape: {x.shape}')\n",
    "output = gpt2_small(x)\n",
    "print(f'Output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63ed17f2-6c70-4f6f-99e2-0186f59be40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model: GPTModel, tokenizer: tiktoken.Encoding, text: str, max_len: int = 1024) -> str:\n",
    "    eos = '<|endoftext|>'\n",
    "    next_token: str | None = None\n",
    "    encoded_len: int = 0\n",
    "    result = text\n",
    "    with torch.no_grad():\n",
    "        while next_token != eos and encoded_len < max_len:\n",
    "            encoded = tokenizer.encode(result, allowed_special={eos})\n",
    "            encoded_len = len(encoded)\n",
    "            next_tok_logits = model(torch.tensor(encoded).unsqueeze(0)).squeeze(0)[-1]\n",
    "            next_tok_probs = torch.softmax(next_tok_logits, dim=0)\n",
    "            next_token = tokenizer.decode([torch.argmax(next_tok_probs).item()])\n",
    "            result += next_token\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb9b15d-4bea-4736-b9af-2a5c8be3324a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,  426 attracted nominations evaluated reviewingouncing'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text(gpt2_small, tokenizer, \"Hello, \", 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
