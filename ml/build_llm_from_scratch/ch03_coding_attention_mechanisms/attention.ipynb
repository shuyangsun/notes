{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aac016e-d730-4100-8d4b-e04e734de03d",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8aad1b-6247-43f3-9f39-af5278183f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ae5fc-1963-459c-a696-f170f8cd6575",
   "metadata": {},
   "source": [
    "## 1. Simplified self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f95d69-ceee-496f-8526-0b45ec2f264c",
   "metadata": {},
   "source": [
    "### Create embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d994da5a-995b-4b2e-9e3c-6577a823fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = '<|endoftext|>'\n",
    "text = \"Your journey starts with one step\" + eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb42f076-efe3-4c3d-ab16-ba649af8c3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7120,  7002,  4940,   351,   530,  2239, 50256])\n"
     ]
    }
   ],
   "source": [
    "data_loader = util.create_dataloader_v1(\n",
    "    text,\n",
    "    batch_size=1,\n",
    "    context_window=6,\n",
    "    stride=7,\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "x, y = next(data_iter)\n",
    "# Need to concat from the last element of the target tensor,\n",
    "# otherwise, setting context window as 4 results in a crash.\n",
    "token_ids = torch.cat((x[0], y[0, -1:]))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee72dd75-2663-4baa-86e8-289af42f0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_len: int = token_ids.size(0)\n",
    "vocab_size: int = torch.max(token_ids).item() + 1\n",
    "embed_dim: int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484ea74a-c673-4e1e-a12b-709e43209c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8311,  1.3393, -1.1653],\n",
      "        [-0.9936,  0.8519, -2.3310],\n",
      "        [ 2.2730,  1.0514, -0.6150],\n",
      "        [ 1.2780, -0.2958, -1.4757],\n",
      "        [-2.9027,  3.0901,  0.6925],\n",
      "        [-0.7583,  0.3646, -0.9988],\n",
      "        [-2.1264,  0.5579, -1.1521]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "tok_embed_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "pos_embed_layer = torch.nn.Embedding(tokens_len, embed_dim)\n",
    "inputs = tok_embed_layer(token_ids) + pos_embed_layer(torch.arange(tokens_len))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec285b-303f-4adb-8849-ec580c51f5d1",
   "metadata": {},
   "source": [
    "### Attention Weight (for \"journey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcae133-b256-45b4-a3c0-d3e4502b2c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([0.0130, 0.8070, 0.0010, 0.0040, 0.0310, 0.0190, 0.1240],\n",
      "       grad_fn=<RoundBackward1>)\n",
      "Sum: tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(tokens_len)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=-1)\n",
    "print(\"Attention weights:\")\n",
    "print(torch.round(attn_weights_2, decimals=3))\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b0df4-cd6a-4cae-9bfe-4ee26d95f9f5",
   "metadata": {},
   "source": [
    "### Attention weight (for all inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3805f301-0e3b-42e0-8b44-a313ecc819d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([[0.3320, 0.1480, 0.3950, 0.0770, 0.0180, 0.0200, 0.0100],\n",
      "        [0.0130, 0.8070, 0.0010, 0.0040, 0.0310, 0.0190, 0.1240],\n",
      "        [0.0640, 0.0010, 0.8960, 0.0380, 0.0000, 0.0010, 0.0000],\n",
      "        [0.1070, 0.0670, 0.3250, 0.4840, 0.0000, 0.0150, 0.0030],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0380, 0.4050, 0.0070, 0.0200, 0.1900, 0.0750, 0.2650],\n",
      "        [0.0010, 0.1030, 0.0000, 0.0000, 0.6370, 0.0100, 0.2490]],\n",
      "       grad_fn=<RoundBackward1>)\n",
      "Sum: tensor(7.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"Attention weights:\")\n",
    "print(torch.round(attn_weights, decimals=3))\n",
    "print(\"Sum:\", attn_weights.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff568fde-95b5-4e57-b79f-f44d3093ee47",
   "metadata": {},
   "source": [
    "### Context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a0c8a1-f71f-4e87-9fa9-0452b24cb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors:\n",
      "tensor([[ 1.0378,  1.0312, -1.1078],\n",
      "        [-1.1537,  0.8782, -2.0441],\n",
      "        [ 2.1363,  1.0175, -0.6857],\n",
      "        [ 1.3623,  0.4056, -1.2118],\n",
      "        [-2.9026,  3.0900,  0.6925],\n",
      "        [-1.5024,  1.1598, -1.2709],\n",
      "        [-2.4877,  2.1992, -0.0969]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attn_weights @ inputs\n",
    "print(\"Context vectors:\")\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13eb5c-377e-4ade-a425-f4b85e4d7b9d",
   "metadata": {},
   "source": [
    "## 2. Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03930ec-50ed-4962-a194-026bc90e7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91aad08d-dddf-474a-8fae-4558b0b7dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519916af-1433-4b2e-ad61-f6b0a4216a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0809, -0.4605], grad_fn=<SqueezeBackward4>)\n",
      "tensor([-1.2738, -1.7953], grad_fn=<SqueezeBackward4>)\n",
      "tensor([-0.8775, -1.3379], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b583220e-7538-4163-badd-3b41d8b6ab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([7, 2])\n",
      "values.shape: torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(f'keys.shape: {keys.shape}')\n",
    "print(f'values.shape: {values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dfccc20-3e8e-424d-9c89-03c6701f23e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2034785747528076\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_2_2 = query_2.dot(keys_2)\n",
    "print(attn_score_2_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9521acf4-3953-4381-b0af-f9e51b6d47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5480,  2.2035, -2.2023, -0.1176,  1.1066,  1.2662,  2.6174],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d2f014-9a17-4039-b542-b39b9bc0a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.0387, 0.2705, 0.0120, 0.0524, 0.1245, 0.1394, 0.3625],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Attention weights sum: 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(f'Attention weights: {attn_weights_2}')\n",
    "print(f'Attention weights sum: {attn_weights_2.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cec4b7f-e5b7-4d1f-b5bb-2769f6651d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7672, -1.1645], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3c132-ecdf-456c-ad57-cecf2913fa1e",
   "metadata": {},
   "source": [
    "## 3. Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09137b93-eb25-40cf-8ef9-eba043a30bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    _wq: torch.Tensor\n",
    "    _wk: torch.Tensor\n",
    "    _wv: torch.Tensor\n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int | None = None):\n",
    "        super().__init__()\n",
    "        if not d_out:\n",
    "            d_out = d_in\n",
    "        self._wq = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self._wk = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self._wv = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        queries = x @ self._wq\n",
    "        keys = x @ self._wk\n",
    "        values = x @ self._wv\n",
    "\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        return attn_weights @ values # context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9cfa77-e2a5-4ba3-b08a-0f5474292bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple context vec shape: torch.Size([7, 2])\n",
      "tensor([[-0.5660,  0.3537],\n",
      "        [-0.9693, -0.0085],\n",
      "        [-0.4474,  0.4220],\n",
      "        [-0.9014, -0.0317],\n",
      "        [-0.6582,  0.7741],\n",
      "        [-0.7078,  0.1000],\n",
      "        [-0.7461,  0.0863]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention_v1 = SelfAttention_v1(d_in, d_out)\n",
    "simple_context_vec = self_attention_v1(inputs)\n",
    "print(f'Simple context vec shape: {simple_context_vec.shape}')\n",
    "print(simple_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65ebc2-c70b-45e5-ba30-9ddb1e67206e",
   "metadata": {},
   "source": [
    "## 2. Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22dbeaea-c449-4936-9f30-967f1f910376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias=False\n",
    "    ): \n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), f'd_out ({d_out}) must be divisible by num_heads ({num_heads})'\n",
    "\n",
    "        self._d_in = d_in\n",
    "        self._d_out = d_out\n",
    "        self._context_length = context_length\n",
    "        self._num_heads = num_heads\n",
    "        self._head_dim = d_out // num_heads\n",
    "\n",
    "        self._w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self._w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self._w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self._out_proj = nn.Linear(d_out, d_out)\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        assert (d_in == self._d_in), f'input d_in ({d_in}) must be the same as d_in ({self._d_in})'\n",
    "\n",
    "        # KQV shape: (b, num_heads, num_tokens, head_dim)\n",
    "        keys = self._reshape_kqv(self._w_key(x), b, num_tokens)\n",
    "        queries = self._reshape_kqv(self._w_query(x), b, num_tokens)\n",
    "        values = self._reshape_kqv(self._w_value(x), b, num_tokens)\n",
    "\n",
    "        # Attention scores and weights shape:  (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attn_weights = torch.softmax(attn_scores / self._head_dim ** 0.5, dim=-1)\n",
    "        attn_weights = self._dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self._d_out)\n",
    "        return self._out_proj(context_vec) # (b, num_tokens, d_out)\n",
    "\n",
    "    def _reshape_kqv(self, tensor: torch.Tensor, b: int, num_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts QKV weights from shape (b, num_tokens, d_out) to (b, num_heads, num_tokens, head_dim).\n",
    "        \"\"\"\n",
    "        return tensor.view(b, num_tokens, self._num_heads, self._head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "547ccad6-8f74-40fe-8834-279d95eae1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vec shape: torch.Size([1, 7, 16])\n",
      "tensor([[[ 2.0074e-01,  1.2954e-01, -2.0794e-01,  5.0414e-02,  4.3644e-01,\n",
      "          -4.2802e-02, -9.0871e-01, -6.8125e-01,  3.1008e-03,  1.2606e-01,\n",
      "          -8.9249e-02,  2.2884e-01,  5.0619e-01,  7.1841e-01,  8.4086e-01,\n",
      "           2.5266e-01],\n",
      "         [ 3.0884e-01,  2.3584e-01, -2.8430e-01,  2.5105e-01,  6.8879e-01,\n",
      "          -1.1523e-01, -1.0889e+00, -7.1455e-01,  1.9503e-01,  1.5492e-01,\n",
      "          -1.3607e-01,  9.3015e-02,  4.4244e-01,  9.9420e-01,  8.6440e-01,\n",
      "           3.3103e-01],\n",
      "         [-1.3366e-01, -1.0432e-01, -5.1187e-02,  2.6272e-02,  8.2505e-02,\n",
      "          -1.2981e-01, -8.9960e-01, -7.9067e-01, -2.6319e-01, -6.0446e-02,\n",
      "          -1.0746e-01,  3.7379e-01,  7.1751e-01,  3.8893e-01,  8.4441e-01,\n",
      "           1.1615e-01],\n",
      "         [ 1.2362e-01,  1.0733e-01, -1.8874e-01,  9.9266e-03,  3.9622e-01,\n",
      "          -6.6299e-02, -9.5984e-01, -8.5638e-01, -7.6979e-05,  1.2348e-02,\n",
      "          -1.4353e-01,  2.6240e-01,  5.9882e-01,  6.8316e-01,  9.2161e-01,\n",
      "           1.9963e-01],\n",
      "         [ 9.8826e-03,  2.2882e-01, -1.6947e-01,  2.8371e-01,  3.6281e-01,\n",
      "          -1.4598e-01, -8.7304e-01, -5.2328e-01, -1.1801e-01,  1.1922e-01,\n",
      "           1.5924e-01,  1.6937e-01,  6.5439e-01,  4.1041e-01,  7.2880e-01,\n",
      "           2.7737e-01],\n",
      "         [ 2.0324e-01,  1.8012e-01, -2.0188e-01,  6.9903e-02,  6.0835e-01,\n",
      "          -9.5447e-02, -9.4647e-01, -7.1662e-01,  5.9056e-02,  8.1817e-02,\n",
      "          -1.1278e-01,  1.7507e-01,  5.3221e-01,  7.8042e-01,  8.9831e-01,\n",
      "           2.3484e-01],\n",
      "         [ 1.5860e-01,  9.9011e-02, -1.0503e-01,  1.7087e-01,  4.7152e-01,\n",
      "          -1.3601e-02, -6.8780e-01, -4.1491e-01,  4.8254e-02,  1.2344e-01,\n",
      "           7.7426e-02,  1.3884e-01,  5.9890e-01,  5.3589e-01,  4.6888e-01,\n",
      "           2.1319e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = 6\n",
    "num_heads = 1\n",
    "mh_attn = MultiHeadAttention(d_in, d_out * num_heads, context_length, dropout=0.1, num_heads=num_heads)\n",
    "example_context_vec = mh_attn(inputs.unsqueeze(0))\n",
    "print(f'Context vec shape: {example_context_vec.shape}')\n",
    "print(example_context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad7b38-10bb-421b-95d8-e05dcb61f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
